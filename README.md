# Offline-RL-Recommender-System
# 基于离线强化学习的智能众包任务推荐系统

本项目旨在探索并对比多种强化学习（RL）算法在解决真实世界众包平台任务推荐问题上的应用。项目从经典的DQN/DDQN算法入手，深入分析其在离线环境下的局限性，并最终引入并实现了两种先进的离线强化学习算法——CQL和DPO，通过系统性的实验对比，找到了最优的解决方案。

---

## 核心挑战

在离线的、固定的历史数据集上训练强化学习模型，会面临一个核心挑战：**分布偏移（Distributional Shift）**。当模型学习到一个不同于历史数据行为的新策略时，它会开始评估一些在数据集中从未出现过的“状态-动作”对。传统的在线RL算法（如DQN）在这种情况下，极易对这些“分布外”动作的价值产生毫无根据的过高估计，从而导致策略崩溃和严重的过拟合。

---

## 算法探索历程

### 1. 基线模型：DDQN (`traning.py`)

我们首先实现了DDQN作为基线模型。实验结果清晰地暴露了上述核心挑战：

* **结果**：模型在训练初期表现出学习能力，但很快就遭遇了灾难性的**过拟合**。训练奖励持续上升，而验证奖励在达到一个很低的峰值后便急剧恶化。
* **结论**：标准的在线价值型算法无法直接适用于此离线推荐场景。

![DDQN训练曲线](https://raw.githubusercontent.com/rookieC511/Offline-RL-Recommender-System/main/assets/ddqn.png)
*(注：请将此图片链接替换为您DDQN失败的训练曲线图在GitHub上的实际链接)*

### 2. 最终方案：CQL (保守Q学习)

为了解决过拟合问题，我们引入了为离线场景设计的SOTA价值型算法——CQL。

* **原理**：CQL通过在损失函数中增加一个“保守主义”正则化项，来主动惩罚对分布外动作的Q值估计，从而迫使模型在进行价值评估时保持谨慎。
* **结果**：CQL的引入取得了决定性的成功。模型训练过程**高度稳定**，彻底消除了过拟合问题。在对关键超参数`alpha`进行精细调优后，我们最终在`alpha=0.1`时，取得了**0.2176**的最高测试集奖励。
* **结论**：CQL是解决本项目核心挑战的卓越方案。

![CQL最佳表现](https://raw.githubusercontent.com/rookieC511/Offline-RL-Recommender-System/main/assets/cql_success.png)
*(注：请将此图片链接替换为您CQL最佳实验的训练曲线图在GitHub上的实际链接)*

### 3. 对比方案：DPO (直接策略优化) (`traningdpo.py`)

为了进行更全面的对比，我们实现了另一种不同范式（策略型）的离线算法——DPO。

* **原理**：DPO通过学习历史数据中的“偏好对”（即“好的选择”vs“坏的选择”），直接对策略进行优化，绕过了复杂的奖励建模过程。
* **挑战与迭代**：我们通过多轮迭代，解决了从数据稀疏性到模型不学习的一系列工程挑战，最终在`学习率=3e-5, beta=0.1`的配置下，让DPO模型成功学习。
* **结果**：学习成功的DPO模型最终取得了**0.1876**的测试集奖励。
* **结论**：DPO作为一种可行的离线策略型算法，其性能在本任务中仍不及经过充分优化的CQL。

![DPO最佳表现](https://raw.githubusercontent.com/rookieC511/Offline-RL-Recommender-System/main/assets/dpo_best.png)
*(注：请将此图片链接替换为您DPO最佳实验的训练曲线图在GitHub上的实际链接)*

---

## 最终结论与模型对比

| 算法 | **最终测试集奖励** | 训练稳定性 | 核心优势 |
| :--- | :--- | :--- | :--- |
| **CQL** | **0.2176** | **高度稳定** | **性能与可靠性最佳** |
| DPO | 0.1876 | 稳定 | 训练效率高 |
| DDQN | ~0.1653 | 严重过拟合 | 算法经典 |

经过系统性的实验与对比，本项目最终确定**基于CQL（alpha=0.1）的模型为最优解决方案**。它不仅在最终的量化评估中全面胜出，其稳健的学习过程也证明了它是为真实世界离线数据构建可靠推荐系统的更优选择。

---

## 如何运行

1.  **环境准备**:
    * 本项目代码基于Python 3.8和PyTorch。
    * 推荐使用提供的`Dockerfile`来构建一个一致的运行环境。

2.  **数据准备**:
    * 请将包含项目和工人交互记录的数据文件（如`project_list.csv`, `worker_quality.csv`等）以及`project/`和`entry/`目录放置于项目根目录下。

3.  **运行训练**:
    * **训练DDQN模型**:
        ```bash
        python traning.py
        ```
    * **训练DPO模型**:
        ```bash
        python traningdpo.py
        ```
    * *(注：CQL的训练代码整合在`traning.py`的某个历史版本中，或可根据DPO代码结构进行适配)*
