import json
import time
import datetime
import math
import csv
import copy
import sys
import os
from dateutil.parser import parse
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque, namedtuple
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, IterableDataset
from collections import namedtuple # ç¡®ä¿è¿™è¡Œ import å­˜åœ¨

# from sklearn.preprocessing import MinMaxScaler, StandardScaler

# --- å¸¸é‡å’Œé…ç½® ---
ACTION_SIZE = 1
##GAMMA = 0.99
LEARNING_RATE =3e-5
BUFFER_SIZE = int(5e4)
BATCH_SIZE = 64
##TAU = 1e-3
##UPDATE_EVERY = 4
REWARD_SCALING_FACTOR = 10  # å®šä¹‰ä¸€ä¸ªå¥–åŠ±ç¼©æ”¾å› å­
##TARGET_UPDATE_EVERY = 100
# --- DPOä¸“å±é…ç½® ---
DPO_BETA = 0.3  # DPOçš„betaè¶…å‚æ•°ï¼Œæ§åˆ¶ç­–ç•¥ä¸éšå¼å‚è€ƒç­–ç•¥çš„åç¦»ç¨‹åº¦ã€‚0.1æ˜¯å¸¸ç”¨åˆå§‹å€¼ã€‚
NUM_REJECTED_SAMPLES = 3 # <-- æ–°å¢ï¼šä¸ºæ¯ä¸ªchosenæ ·æœ¬åŒ¹é…3ä¸ªrejectedæ ·æœ¬
CATEGORY_EMBED_DIM = 5
SUB_CATEGORY_EMBED_DIM = 8
INDUSTRY_EMBED_DIM = 5
WORKER_ID_EMBED_DIM = 8
# <--- å…³é”®ä¿®æ”¹: ä¸ºæ–°ç‰¹å¾æ›´æ–°å¸¸é‡ ---
##CQL_ALPHA = 0.15  # CQLæŸå¤±çš„æƒé‡ã€‚è¿™æ˜¯ä¸€ä¸ªéœ€è¦è°ƒä¼˜çš„è¶…å‚æ•°ï¼Œå¯ä»¥ä»0.5, 1.0, 2.0, 5.0ç­‰å¼€å§‹å°è¯•ã€‚
NUM_NUMERIC_WORKER_FEATURES = 5
NUM_NUMERIC_BASE_PROJECT_FEATURES = 4
NUM_NUMERIC_INTERACTION_FEATURES = 4 # æ–°å¢çš„â€œå·¥äººç±»åˆ«å†å²è¡¨ç°â€ç‰¹å¾
NUM_NUMERIC_CONTEXT_FEATURES = 1
TOTAL_NUMERIC_FEATURES = NUM_NUMERIC_WORKER_FEATURES + NUM_NUMERIC_BASE_PROJECT_FEATURES + NUM_NUMERIC_INTERACTION_FEATURES + NUM_NUMERIC_CONTEXT_FEATURES


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"å½“å‰ä½¿ç”¨è®¾å¤‡: {device}")

all_begin_time_dt = parse("2018-01-01T0:0:0Z")
feature_min_max = {}
REWARD_SCALE_REFERENCE = 20.0  # å…ˆç»™ä¸€ä¸ªé»˜è®¤å€¼ï¼Œä¼šè¢«åç»­åˆ†æè¦†ç›–

# --- æ•°æ®åŠ è½½ ---
worker_quality_map = {}
project_list_data = {}
project_info = {}
entry_info = {}
industry_map = {}
industry_counter = 0
try:
    with open("worker_quality.csv", "r", encoding='utf-8') as csvfile:
        csvreader = csv.reader(csvfile)
        next(csvreader)
        for line in csvreader:
            try:
                worker_id, quality_str = line[0], line[1]
                quality = float(quality_str)
                if quality > 0.0: worker_quality_map[int(worker_id)] = quality / 100.0
            except (ValueError, IndexError):
                pass  # é™é»˜å¤„ç†æ ¼å¼é”™è¯¯è¡Œ
except FileNotFoundError:
    print("è­¦å‘Š: æœªæ‰¾åˆ° worker_quality.csvã€‚")
try:
    with open("project_list.csv", "r", encoding='utf-8') as file:
        project_list_lines = file.readlines()
        for line in project_list_lines[1:]:
            parts = line.strip('\n').split(',')
            try:
                project_list_data[int(parts[0])] = int(parts[1])
            except (IndexError, ValueError):
                pass
except FileNotFoundError:
    print("è­¦å‘Š: æœªæ‰¾åˆ° project_list.csvã€‚")
project_dir, entry_dir = "project/", "entry/"
project_files = [f for f in os.listdir(project_dir) if
                 f.startswith("project_") and f.endswith(".txt")] if os.path.exists(project_dir) else []
for project_filename in project_files:
    if project_filename == ".DS_Store": continue
    try:
        project_id = int(project_filename.replace("project_", "").replace(".txt", ""))
        if project_id not in project_list_data: continue
        with open(os.path.join(project_dir, project_filename), "r", encoding='utf-8') as file:
            text = json.load(file)
        project_info[project_id] = {"id": project_id, "sub_category": int(text.get("sub_category", -1)),
                                    "category": int(text.get("category", -1)),
                                    "start_date_dt": parse(text["start_date"]), "deadline_dt": parse(text["deadline"]),
                                    "total_awards": float(text.get("total_awards", 0.0)),
                                    "status": text.get("status", "unknown"),
                                    "required_answers": project_list_data.get(project_id, 1)}
        industry_str = text.get("industry", "unknown_industry");
        if industry_str not in industry_map: industry_map[industry_str] = industry_counter; industry_counter += 1
        project_info[project_id]["industry_id"] = industry_map[industry_str]
        entry_info[project_id] = {}
        page_k = 0
        while True:
            entry_filename = os.path.join(entry_dir, f"entry_{project_id}_{page_k}.txt")
            if not os.path.exists(entry_filename): break
            try:
                with open(entry_filename, "r", encoding='utf-8') as efile:
                    entry_text_data = json.load(efile)
                for item in entry_text_data.get("results", []):                 score_val = 0 # é»˜è®¤åˆ†æ•°
                if item.get("revisions") and isinstance(item["revisions"], list) and len(item["revisions"]) > 0:
                    # ç¡®ä¿ revisions[0] æ˜¯ä¸€ä¸ªå­—å…¸å¹¶ä¸”åŒ…å« "score"
                    if isinstance(item["revisions"][0], dict):
                        score_val = item["revisions"][0].get("score", 0) # å¦‚æœæ²¡æœ‰scoreé”®ï¼Œé»˜è®¤ä¸º0

                entry_info[project_id][int(item["entry_number"])] = {
                    "entry_created_at_dt": parse(item.get("entry_created_at", "1970-01-01T00:00:00Z")), # æ·»åŠ é»˜è®¤å€¼ä»¥é˜²ä¸‡ä¸€
                    "worker_id": int(item["author"]),
                    "withdrawn": item.get("withdrawn", False),
                    "award_value": item.get("award_value"),
                    "score": score_val, # <--- æ–°å¢æˆ–ç¡®ä¿æ­£ç¡®èµ‹å€¼
                    "winner": item.get("winner", False) # <--- åŒæ—¶ç¡®ä¿winnerä¹Ÿè¢«åŠ è½½
                }
            except Exception:
                pass  # é™é»˜å¤„ç†
            page_k += 24
    except Exception:
        pass  # é™é»˜å¤„ç†
# ç¤ºä¾‹æ€§çš„æ•°æ®åŠ è½½ç»“æŸ
print(
    f"æ•°æ®è¯»å–å®Œæˆã€‚é¡¹ç›®æ•°: {len(project_info)}, Entryé¡¹ç›®æ•°: {len(entry_info)}, å·¥äººè´¨é‡è®°å½•æ•°: {len(worker_quality_map)}, è¡Œä¸šæ•°: {len(industry_map if industry_map else [])}")

# --- å†å²å¥–åŠ±ç»Ÿè®¡åˆ†æ ---
# ... (ç²˜è´´å†å²å¥–åŠ±ç»Ÿè®¡åˆ†æä»£ç ï¼Œå®ƒå¯ä»¥ä¿®æ”¹å…¨å±€çš„ REWARD_SCALE_REFERENCE) ...
print("\n--- å¼€å§‹å†å²å¥–åŠ± (award_value > 0) ç»Ÿè®¡åˆ†æ ---")
all_historical_awards = []  # ... (å…¶ä½™ç»Ÿè®¡ä»£ç å¦‚å‰ä¸€ä¸ªå›å¤æ‰€ç¤º) ...
# ... (ç¡®ä¿ REWARD_SCALE_REFERENCE åœ¨è¿™é‡Œè¢«æ­£ç¡®èµ‹å€¼) ...
if entry_info:
    for project_id, entries_in_project in entry_info.items():
        for entry_number, entry_data in entries_in_project.items():
            award_val_raw = entry_data.get("award_value")
            if award_val_raw is not None:
                try:
                    award_val_float = float(award_val_raw)
                    if award_val_float > 0: all_historical_awards.append(award_val_float)
                except (ValueError, TypeError):
                    pass
if all_historical_awards:
    awards_array = np.array(all_historical_awards)
    REWARD_SCALE_REFERENCE = np.median(awards_array)
    if REWARD_SCALE_REFERENCE <= 0: REWARD_SCALE_REFERENCE = np.mean(awards_array) if np.mean(
        awards_array) > 0 else 20.0
    print(f"å»ºè®®ç”¨äºå¥–åŠ±ç¼©æ”¾çš„å‚è€ƒå€¼ (REWARD_SCALE_REFERENCE): {REWARD_SCALE_REFERENCE:.2f}")
else:
    print("æ•°æ®é›†ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ­£å‘å†å²å¥–åŠ±ã€‚ä½¿ç”¨é»˜è®¤ç¼©æ”¾å‚è€ƒã€‚")
    REWARD_SCALE_REFERENCE = 20.0  # ä¿æŒé»˜è®¤
print("--- å†å²å¥–åŠ±ç»Ÿè®¡åˆ†æç»“æŸ ---\n")
# --- åœ¨æ•°æ®åŠ è½½åï¼Œè¿›è¡Œæ–°çš„é¢„è®¡ç®— ---
print("\n--- å¼€å§‹æ„å»ºä¸°å¯Œçš„å·¥äººå…¨å±€åŠäº¤äº’ç‰¹å¾ ---")

# ç”¨äºå­˜å‚¨å…¨å±€ç”»åƒ
worker_global_stats = {} 
# worker_global_stats[worker_id] = {'total_score': float, 'count': int, 'wins': int, 'categories': set()}

# ç”¨äºå­˜å‚¨äº¤äº’ç‰¹å¾ (å‡çº§ç‰ˆ)
worker_cat_performance = {} 
# worker_cat_performance[(worker_id, cat_id)] = {'total_score': float, 'count': int, 'wins': int}

if entry_info:
    for proj_id, entries in entry_info.items():
        proj_details = project_info.get(proj_id)
        if not proj_details: continue
        
        proj_cat = proj_details.get("category")
        if proj_cat is None: continue

        for _, entry_data in entries.items():
            worker_id = entry_data["worker_id"]
            score = entry_data.get("score", 0)
            is_winner = entry_data.get("winner", False)

            # --- æ›´æ–°äº¤äº’ç‰¹å¾ ---
            key = (worker_id, proj_cat)
            if key not in worker_cat_performance:
                worker_cat_performance[key] = {'total_score': 0.0, 'count': 0, 'wins': 0}
            
            worker_cat_performance[key]['total_score'] += score
            worker_cat_performance[key]['count'] += 1
            if is_winner:
                worker_cat_performance[key]['wins'] += 1

            # --- æ›´æ–°å…¨å±€ç”»åƒ ---
            if worker_id not in worker_global_stats:
                worker_global_stats[worker_id] = {'total_score': 0.0, 'count': 0, 'wins': 0, 'categories': set()}

            worker_global_stats[worker_id]['total_score'] += score
            worker_global_stats[worker_id]['count'] += 1
            worker_global_stats[worker_id]['categories'].add(proj_cat)
            if is_winner:
                worker_global_stats[worker_id]['wins'] += 1

print(f"å®Œæˆã€‚å…±è®°å½•äº† {len(worker_global_stats)} ä½å·¥äººçš„å…¨å±€ç”»åƒå’Œ {len(worker_cat_performance)} æ¡äº¤äº’è¡¨ç°ã€‚")
# --- åŠ¨æ€ç¡®å®šåˆ†ç±»ç‰¹å¾çš„åŸºæ•° ---
# ... (ç²˜è´´åŠ¨æ€ç¡®å®šåŸºæ•°ä»£ç ï¼Œç¡®ä¿ NUM_..._EMBED_SIZE è¢«æ­£ç¡®èµ‹å€¼) ...
max_cat_id, max_sub_cat_id, max_worker_id, max_industry_id = 0, 0, 0, 0
if project_info: max_cat_id = max(
    p["category"] for p in project_info.values() if "category" in p); max_sub_cat_id = max(
    p["sub_category"] for p in project_info.values() if "sub_category" in p); max_industry_id = max(
    p["industry_id"] for p in project_info.values() if "industry_id" in p)
if entry_info: all_workers_set = set(
    ed["worker_id"] for entries in entry_info.values() for ed in entries.values()); max_worker_id = max(
    all_workers_set) if all_workers_set else 0
NUM_CATEGORIES_EMBED_SIZE = max_cat_id + 1;
NUM_SUB_CATEGORIES_EMBED_SIZE = max_sub_cat_id + 1;
NUM_INDUSTRIES_EMBED_SIZE = max_industry_id + 1;
NUM_WORKERS_EMBED_SIZE = max_worker_id + 1
print(
    f"åµŒå…¥å±‚å¤§å°: Cat={NUM_CATEGORIES_EMBED_SIZE}, SubCat={NUM_SUB_CATEGORIES_EMBED_SIZE}, Ind={NUM_INDUSTRIES_EMBED_SIZE}, Worker={NUM_WORKERS_EMBED_SIZE}")

# --- ç‰¹å¾å½’ä¸€åŒ–ï¼šé¢„è®¡ç®— Min/Max ---
# ... (ç²˜è´´ precompute_feature_min_max å’Œ min_max_scale å‡½æ•°å®šä¹‰) ...
def get_available_projects(current_time_dt, project_info_map, entry_info_map):  # ç¡®ä¿æ­¤å‡½æ•°å·²å®šä¹‰
    available = []
    for pid, p_data in project_info_map.items():
        if p_data.get("start_date_dt") <= current_time_dt and p_data.get(
                "deadline_dt") > current_time_dt and p_data.get("status", "open").lower() != "completed":
            accepted_count = 0
            if pid in entry_info_map:
                for _, e_data in entry_info_map[pid].items():
                    if not e_data.get("withdrawn", False) and e_data.get(
                        "entry_created_at_dt") <= current_time_dt: accepted_count += 1
            if accepted_count < p_data.get("required_answers", 1): available.append(pid)
    return available


def precompute_feature_min_max(arrival_events, proj_info, ent_info, wq_map):  # precompute å‡½æ•°å®šä¹‰
    global feature_min_max;
    print("å¼€å§‹é¢„è®¡ç®—ç‰¹å¾ Min/Max...")
    data_to_scale = {name: [] for name in
                     ["worker_quality", "time_until_deadline_sec", "task_age_sec", "project_duration_sec",
                      "reward_per_slot", "current_time_val"]}
    for event in arrival_events:
        worker_id, current_time = event['worker_id'], event['arrival_time_dt']
        data_to_scale["worker_quality"].append(wq_map.get(worker_id, 0.0))
        data_to_scale["current_time_val"].append((current_time - all_begin_time_dt).total_seconds())
        available_p_ids = get_available_projects(current_time, proj_info, ent_info)
        for p_id in available_p_ids:
            p_data = proj_info.get(p_id)
            if p_data:
                data_to_scale["time_until_deadline_sec"].append((p_data["deadline_dt"] - current_time).total_seconds())
                data_to_scale["task_age_sec"].append((current_time - p_data["start_date_dt"]).total_seconds())
                dur_sec = (p_data["deadline_dt"] - p_data["start_date_dt"]).total_seconds();
                data_to_scale["project_duration_sec"].append(max(0, dur_sec))
                rps = (p_data["total_awards"] / p_data["required_answers"]) if p_data.get("required_answers",
                                                                                          0) > 0 and p_data.get(
                    "total_awards", 0) > 0 else 0
                data_to_scale["reward_per_slot"].append(rps)
    for name, values in data_to_scale.items():
        if values:
            min_v, max_v = np.min(values), np.max(values)
            if min_v == max_v: max_v = min_v + 1e-6 if min_v == 0 else min_v * 1.01;
            if min_v == max_v: min_v, max_v = 0.0, 1.0
            feature_min_max[name] = (min_v, max_v)
            # print(f"ç‰¹å¾ '{name}': Min={min_v:.2f}, Max={max_v:.2f}") # å¯ä»¥æ³¨é‡Šæ‰
        else:
            feature_min_max[name] = (0, 1)
    print("ç‰¹å¾ Min/Max é¢„è®¡ç®—å®Œæˆã€‚")


def min_max_scale(value, feature_name):  # min_max_scale å‡½æ•°å®šä¹‰
    min_val, max_val = feature_min_max.get(feature_name, (0, 1))
    if max_val == min_val: return 0.5
    return np.clip((value - min_val) / (max_val - min_val), 0.0, 1.0)  # æ·»åŠ clipç¡®ä¿åœ¨0-1


# --- ç‰¹å¾å·¥ç¨‹å‡½æ•° ---
# ... (ç²˜è´´ get_worker_features_simplified, get_project_features_simplified, get_context_features_simplified å‡½æ•°å®šä¹‰) ...
def get_worker_features_simplified(worker_id, wq_map):  # get_worker_features_simplified å®šä¹‰
    quality_raw = wq_map.get(worker_id, 0.0)
    quality_scaled = min_max_scale(quality_raw, "worker_quality")
    # ä¸ºäº†è°ƒè¯•å½’ä¸€åŒ–ï¼Œå¯ä»¥æš‚æ—¶åœ¨è¿™é‡Œæ‰“å°
    #if random.random() < 0.001: # éšæœºæ‰“å°ä¸€å°éƒ¨åˆ†ï¼Œé¿å…è¾“å‡ºè¿‡å¤š
         #print(f"DEBUG_WQ: raw_quality={quality_raw:.2f}, scaled_quality={quality_scaled:.4f}")
    return worker_id, np.array([quality_scaled])


def get_project_features_simplified(project_id, proj_data_map, current_time):  # get_project_features_simplified å®šä¹‰
    p_data = proj_data_map.get(project_id)
    if not p_data: return 0, 0, 0, None
    cat_id, sub_cat_id, ind_id = max(0, p_data.get("category", 0)), max(0, p_data.get("sub_category", 0)), max(0,
                                                                                                               p_data.get(
                                                                                                                   "industry_id",
                                                                                                                   0))
    time_until_deadline_raw = (p_data["deadline_dt"] - current_time).total_seconds()
    task_age_raw = (current_time - p_data["start_date_dt"]).total_seconds()
    project_duration_raw = max(0, (p_data["deadline_dt"] - p_data["start_date_dt"]).total_seconds())
    reward_per_slot_raw = (p_data["total_awards"] / p_data["required_answers"]) if p_data.get("required_answers",
                                                                                              0) > 0 and p_data.get(
        "total_awards", 0) > 0 else 0
    numeric_project_f = np.array(
        [min_max_scale(time_until_deadline_raw, "time_until_deadline_sec"), min_max_scale(task_age_raw, "task_age_sec"),
         min_max_scale(project_duration_raw, "project_duration_sec"),
         min_max_scale(reward_per_slot_raw, "reward_per_slot")])

    # ä¸ºäº†è°ƒè¯•å½’ä¸€åŒ–
    #if random.random() < 0.001:
         #print(f"DEBUG_PF_RAW: ProjID:{project_id} T_Deadline={time_until_deadline_raw:.0f}, T_Age={task_age_raw:.0f}, T_Dur={project_duration_raw:.0f}, RPS={reward_per_slot_raw:.2f}")
         #print(f"DEBUG_PF_SCA: ProjID:{project_id} Scaled_Feats={np.array2string(numeric_project_f, formatter={'float_kind':lambda x: '%.4f' % x})}")
    return cat_id, sub_cat_id, ind_id, numeric_project_f


def get_context_features_simplified(current_time):
    current_time_raw = (current_time - all_begin_time_dt).total_seconds()
    current_time_scaled_value = min_max_scale(current_time_raw, "current_time_val")  # å…ˆè®¡ç®—ç¼©æ”¾åçš„å•ä¸ªå€¼

    # ä¸ºäº†è°ƒè¯•å½’ä¸€åŒ– (å¯é€‰ï¼Œå¦‚æœéœ€è¦æ‰“å°)
    #if random.random() < 0.001: # æˆ–è€…ä½ å¯ä»¥ç§»é™¤è¿™ä¸ªéšæœºæ¡ä»¶ï¼Œåœ¨è®­ç»ƒå¾ªç¯å¤–æ§åˆ¶æ‰“å°é¢‘ç‡
         #print(f"DEBUG_CF: raw_time={current_time_raw:.0f}, scaled_time={current_time_scaled_value:.4f}")

    return np.array([current_time_scaled_value])  # å°†ç¼©æ”¾åçš„å•ä¸ªå€¼æ”¾å…¥Numpyæ•°ç»„ä¸­è¿”å›
# --- å®šä¹‰æ–°çš„çŠ¶æ€ç”Ÿæˆå‡½æ•° ---

def get_new_final_state_tuple(worker_id, project_id, current_time, 
                              proj_info_map, wq_map, global_stats, cat_perf_map):
    
    # --- 1. è·å–é¡¹ç›®åŸºç¡€ç‰¹å¾ (ä¸å˜) ---
    cat_id, sub_cat_id, ind_id, numeric_project_f_base = get_project_features_simplified(project_id, proj_info_map, current_time)
    if numeric_project_f_base is None: return None

    # --- 2. è·å–å·¥äººå…¨å±€ç”»åƒç‰¹å¾ ---
    #    (è¿™é‡Œéœ€è¦è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¯ä»¥é¢„è®¡ç®—min/maxï¼Œæˆ–ä½¿ç”¨ä¼°è®¡å€¼)
    worker_stats = global_stats.get(worker_id, {})
    
    # åŸå§‹è´¨é‡åˆ† (æ¥è‡ªworker_quality.csv)
    worker_quality_scaled = min_max_scale(wq_map.get(worker_id, 0.0), "worker_quality") 
    
    # å…¨å±€æ€»å‚ä¸æ¬¡æ•° (ç®€å•å½’ä¸€åŒ–ï¼Œæ¯”å¦‚é™¤ä»¥50)
    global_participation_count = worker_stats.get('count', 0)
    global_participation_scaled = np.clip(global_participation_count / 50.0, 0, 1)

    # å…¨å±€å¹³å‡åˆ† (0-5åˆ† -> 0-1)
    global_avg_score = (worker_stats.get('total_score', 0) / global_participation_count) if global_participation_count > 0 else 0
    global_avg_score_scaled = global_avg_score / 5.0
    
    # å…¨å±€èƒœç‡
    global_win_rate = (worker_stats.get('wins', 0) / global_participation_count) if global_participation_count > 0 else 0
    
    # å‚ä¸ç±»åˆ«çš„å¤šæ ·æ€§ (ç®€å•å½’ä¸€åŒ–ï¼Œæ¯”å¦‚é™¤ä»¥10)
    category_diversity = len(worker_stats.get('categories', set()))
    category_diversity_scaled = np.clip(category_diversity / 10.0, 0, 1)

    numeric_worker_global_f = np.array([
        worker_quality_scaled, 
        global_participation_scaled, 
        global_avg_score_scaled, 
        global_win_rate, 
        category_diversity_scaled
    ])

    # --- 3. è·å–å·¥äºº-ç±»åˆ«äº¤äº’ç‰¹å¾ ---
    key = (worker_id, cat_id)
    cat_stats = cat_perf_map.get(key, {})
    
    cat_participation_count = cat_stats.get('count', 0)
    
    # åœ¨è¯¥ç±»åˆ«ä¸‹çš„å¹³å‡åˆ†
    cat_avg_score = (cat_stats.get('total_score', 0) / cat_participation_count) if cat_participation_count > 0 else 0
    cat_avg_score_scaled = cat_avg_score / 5.0
    
    # åœ¨è¯¥ç±»åˆ«ä¸‹çš„å‚ä¸æ¬¡æ•°
    cat_participation_scaled = np.clip(cat_participation_count / 10.0, 0, 1) # é™¤ä»¥10å½’ä¸€åŒ–
    
    # åœ¨è¯¥ç±»åˆ«ä¸‹çš„èƒœç‡
    cat_win_rate = (cat_stats.get('wins', 0) / cat_participation_count) if cat_participation_count > 0 else 0
    
    # æ˜¯å¦æ˜¯æ–°æ‰‹ (é‡è¦ä¿¡å·)
    is_new_to_category = 1.0 if cat_participation_count == 0 else 0.0

    numeric_interaction_f = np.array([
        cat_avg_score_scaled, 
        cat_participation_scaled, 
        cat_win_rate,
        is_new_to_category
    ])

    # --- 4. è·å–ä¸Šä¸‹æ–‡ç‰¹å¾ (ä¸å˜) ---
    numeric_context_f = get_context_features_simplified(current_time)

    # --- 5. æ‹¼æ¥æ‰€æœ‰æ•°å€¼ç‰¹å¾ ---
    final_numeric_features = np.concatenate([
        numeric_worker_global_f,
        numeric_project_f_base,
        numeric_interaction_f,
        numeric_context_f
    ])
    
    # æ³¨æ„ï¼šè¿”å›å€¼ä¸å†åŒ…å« worker_id_embed
    return (max(0, cat_id), max(0, sub_cat_id), max(0, ind_id), final_numeric_features)
# æ›¿æ¢æ‰æˆ–è€…æ³¨é‡Šæ‰åŸæ¥çš„Experience namedtuple
PreferencePair = namedtuple("PreferencePair", 
                            field_names=["chosen_state_tuple", "rejected_state_tuple"])
# --- QNetwork, ReplayBuffer, DQNAgentWithEmbeddings ç±»å®šä¹‰ ---
# ... (ç²˜è´´ä½ å·²éªŒè¯æ— è¯¯çš„è¿™ä¸‰ä¸ªç±»çš„å®Œæ•´å®šä¹‰) ...
# (ç¡®ä¿ ReplayBuffer.sample å’Œ DQNAgentWithEmbeddings.act/learn/step æ­£ç¡®å¤„ç†ç‰¹å¾å…ƒç»„)
# (æ­¤å¤„ç²˜è´´ä¸Šä¸€å›å¤ä¸­çš„ PolicyNetwork, ReplayBuffer, DQNAgentWithEmbeddings ç±»å®šä¹‰)
# åœ¨ get_new_final_state_tuple å‡½æ•°ä¹‹åæ·»åŠ 

def create_preference_dataset_v4(events, proj_info, ent_info, wq_map, global_stats, cat_perf_map):
    """
    ã€V4æœ€ç»ˆç‰ˆã€‘
    æ”¾å®½ "Chosen" çš„æ ‡å‡†ï¼Œä»¥ç”Ÿæˆè¶³å¤Ÿå¤šçš„é«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚
    """
    print(f"å¼€å§‹ä¸º {len(events)} ä¸ªäº‹ä»¶åˆ›å»ºåå¥½æ•°æ®é›† (V4ç‰ˆæœ¬)...")
    preference_data = []
    
    for event in events:
        current_worker_id = event['worker_id']
        current_time_dt = event['arrival_time_dt']
        chosen_project_id = event['project_id']
        chosen_entry_num = event['entry_number']
        
        # --- æ ¸å¿ƒä¼˜åŒ–ç‚¹ï¼šæ”¾å®½ "Chosen" çš„æ ‡å‡† ---
        try:
            entry_details = ent_info[chosen_project_id][chosen_entry_num]
            is_winner = entry_details.get("winner", False)
            score = entry_details.get("score", 0)
            
            # å¤„ç† award_value å¯èƒ½ä¸º None çš„æƒ…å†µ
            award_val_raw = entry_details.get("award_value")
            has_award = False
            if award_val_raw is not None:
                try:
                    has_award = float(award_val_raw) > 0
                except (ValueError, TypeError):
                    pass # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œåˆ™è®¤ä¸ºæ²¡æœ‰å¥–åŠ±

        except KeyError:
            continue

        # ä½¿ç”¨æ–°çš„ã€æ›´å®½æ¾çš„æ¡ä»¶
        if not (is_winner or score >= 4 or has_award):
            continue # å¦‚æœä¸æ˜¯è·èƒœè€…ã€é«˜åˆ†è€…æˆ–è·å¥–è€…ï¼Œåˆ™è·³è¿‡

        # å¯»æ‰¾"Rejected"æ ·æœ¬ (é€»è¾‘ä¸å˜)
        available_project_ids = get_available_projects(current_time_dt, proj_info, ent_info)
        rejected_candidates = [pid for pid in available_project_ids if pid != chosen_project_id]
        
        if not rejected_candidates:
            continue

        num_to_sample = min(NUM_REJECTED_SAMPLES, len(rejected_candidates))
        if num_to_sample == 0:
            continue

        sampled_rejected_ids = random.sample(rejected_candidates, k=num_to_sample)
        
        # ä¸ºæ¯ä¸ªé…å¯¹ç”ŸæˆçŠ¶æ€å…ƒç»„
        for rejected_project_id in sampled_rejected_ids:
            chosen_state_tuple = get_new_final_state_tuple(
                current_worker_id, chosen_project_id, current_time_dt,
                proj_info, wq_map, global_stats, cat_perf_map
            )
            rejected_state_tuple = get_new_final_state_tuple(
                current_worker_id, rejected_project_id, current_time_dt,
                proj_info, wq_map, global_stats, cat_perf_map
            )
            
            if chosen_state_tuple and rejected_state_tuple:
                pair = PreferencePair(chosen_state_tuple, rejected_state_tuple)
                preference_data.append(pair)
            
    print(f"æˆåŠŸåˆ›å»º {len(preference_data)} æ¡åå¥½æ•°æ®ã€‚")
    return preference_data
class PolicyNetwork(nn.Module):
    """ç¥ç»ç½‘ç»œæ¨¡å‹ (Q-Network)"""
    def __init__(self, num_categories_embed_size, num_sub_categories_embed_size,
                 num_industries_embed_size, cat_embed_dim, sub_cat_embed_dim, ind_embed_dim,
                 total_numeric_features, seed, fc1_units=128, fc2_units=32):
        
        super(PolicyNetwork, self).__init__()
        self.seed = torch.manual_seed(seed)
        
        # å®šä¹‰åµŒå…¥å±‚
        self.category_embedding = nn.Embedding(num_categories_embed_size, cat_embed_dim)
        self.sub_category_embedding = nn.Embedding(num_sub_categories_embed_size, sub_cat_embed_dim)
        self.industry_embedding = nn.Embedding(num_industries_embed_size, ind_embed_dim)
        
        # è®¡ç®—å…¨è¿æ¥å±‚çš„è¾“å…¥ç»´åº¦
        total_embed_dim =cat_embed_dim + sub_cat_embed_dim + ind_embed_dim
        fc_input_dim = total_embed_dim + total_numeric_features
        
        # å®šä¹‰å…¨è¿æ¥å±‚å’ŒDropoutå±‚
        self.fc1 = nn.Linear(fc_input_dim, fc1_units)
        self.dropout1 = nn.Dropout(0.4)
        self.fc2 = nn.Linear(fc1_units, fc2_units)
        self.dropout2 = nn.Dropout(0.4)
        self.fc3 = nn.Linear(fc2_units, 1)

    def forward(self, id_features_batch, numeric_features_batch):
        # åˆ†å‰²IDç‰¹å¾
        cat_ids, sub_cat_ids, ind_ids = id_features_batch.split(1, dim=1)
        
        # è·å–åµŒå…¥å‘é‡
        cat_embed = self.category_embedding(cat_ids.squeeze(-1))
        sub_cat_embed = self.sub_category_embedding(sub_cat_ids.squeeze(-1))
        ind_embed = self.industry_embedding(ind_ids.squeeze(-1))
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        embedded_features = torch.cat([cat_embed, sub_cat_embed, ind_embed], dim=1)
        all_features = torch.cat([embedded_features, numeric_features_batch], dim=1)
        
        # é€šè¿‡ç½‘ç»œå±‚è¿›è¡Œå‰å‘ä¼ æ’­
        x = F.relu(self.fc1(all_features))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        return self.fc3(x)


Experience = namedtuple("Experience",
                        field_names=["state_tuple", "action_project_id", "reward", "next_state_options_tuples", "done"])
# 3. å®šä¹‰å…¨æ–°çš„ DPOAgent
class DPOAgent:
    """
    ä½¿ç”¨DPOç®—æ³•è¿›è¡Œå­¦ä¹ çš„æ™ºèƒ½ä½“ã€‚
    """
    def __init__(self, num_categories_embed_size, num_sub_categories_embed_size,
                 num_industries_embed_size, cat_embed_dim, sub_cat_embed_dim, ind_embed_dim,
                 total_numeric_features, seed):
        
        # DPOåªæœ‰ä¸€ä¸ªç­–ç•¥ç½‘ç»œï¼Œæ²¡æœ‰ç›®æ ‡ç½‘ç»œ
        policy_network_base = PolicyNetwork(
            num_categories_embed_size, num_sub_categories_embed_size,
            num_industries_embed_size, cat_embed_dim, sub_cat_embed_dim,
            ind_embed_dim, total_numeric_features, seed
        ).to(device)

        if torch.cuda.device_count() > 1:
            print(f"æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPUï¼Œå°†ä½¿ç”¨ DataParallelã€‚")
            self.policy_network = nn.DataParallel(policy_network_base)
        else:
            self.policy_network = policy_network_base
            
        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=LEARNING_RATE)

    def learn(self, batch_of_pairs):
        """
        ä½¿ç”¨ä¸€ä¸ªæ‰¹æ¬¡çš„åå¥½å¯¹æ¥æ›´æ–°ç­–ç•¥ç½‘ç»œã€‚
        """
        chosen_states, rejected_states = batch_of_pairs
        
        # åˆ†ç¦»IDå’Œæ•°å€¼ç‰¹å¾
        chosen_id_feats, chosen_num_feats = chosen_states
        rejected_id_feats, rejected_num_feats = rejected_states
        
        # è®¡ç®—åå¥½åˆ†å€¼ (logits)
        chosen_logits = self.policy_network(chosen_id_feats, chosen_num_feats)
        rejected_logits = self.policy_network(rejected_id_feats, rejected_num_feats)
        
        # DPO æŸå¤±å‡½æ•°çš„æ ¸å¿ƒ
        log_probs_diff = chosen_logits - rejected_logits
        loss = -F.logsigmoid(DPO_BETA * log_probs_diff).mean()
        
        # æ¢¯åº¦æ›´æ–°
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), 1.0)
        self.optimizer.step()
        
        return loss.item()

    def act(self, state_options_with_id, eps=0.):
        """
        æ ¹æ®å­¦åˆ°çš„ç­–ç•¥é€‰æ‹©åŠ¨ä½œ (é¡¹ç›®)ã€‚
        """
        # DPOæ˜¯ç¡®å®šæ€§ç­–ç•¥ï¼Œepså‚æ•°å¯ä»¥å¿½ç•¥ï¼Œä½†ä¿ç•™ä»¥å…¼å®¹è¯„ä¼°å‡½æ•°
        if not state_options_with_id: return None
        
        self.policy_network.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        with torch.no_grad():
            all_id_features, all_numeric_features = [], []
            for pid, s_tup in state_options_with_id:
                id_features = [s_tup[0], s_tup[1], s_tup[2]]
                numeric_features = s_tup[3]
                all_id_features.append(id_features)
                all_numeric_features.append(numeric_features)
            
            id_features_tensor = torch.LongTensor(all_id_features).to(device)
            numeric_features_tensor = torch.FloatTensor(np.array(all_numeric_features)).to(device)
            
            # è·å¾—æ‰€æœ‰é€‰é¡¹çš„åå¥½åˆ†
            logits = self.policy_network(id_features_tensor, numeric_features_tensor)
            
            # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„åŠ¨ä½œ
            action_idx = torch.argmax(logits).item()
        
        self.policy_network.train() # æ¢å¤è®­ç»ƒæ¨¡å¼
        return state_options_with_id[action_idx][0]


def calculate_unified_reward(chosen_project_id, current_worker_id, chosen_project_state_tuple,
                             project_info_map, entry_info_map, 
                             scale_reference, scaling_factor):
    """
    ä¸€ä¸ªç»Ÿä¸€çš„ã€æ ‡å‡†çš„å¥–åŠ±è®¡ç®—å‡½æ•°ã€‚
    å®ƒè®¡ç®—ä¸€ä¸ªåŸºç¡€å¥–åŠ±ï¼Œç„¶ååº”ç”¨ç¼©æ”¾å› å­ã€‚
    """
    
    # --- 1. æŸ¥æ‰¾å†å²è¡¨ç° ---
    actual_award_hist = 0.0
    hist_score = 0
    is_winner_hist = False
    hq_award_hist = False
    worker_participated_hist = False
    
    if chosen_project_id in entry_info_map:
        for _, ed in entry_info_map[chosen_project_id].items():
            if ed["worker_id"] == current_worker_id and not ed.get("withdrawn", False):
                worker_participated_hist = True
                current_score_val = ed.get("score", 0)
                if current_score_val > hist_score:
                    hist_score = current_score_val
                
                award_val_raw = ed.get("award_value")
                if award_val_raw is not None:
                    try:
                        award_val_float = float(award_val_raw)
                        if award_val_float > 0:
                            hq_award_hist = True
                            actual_award_hist = award_val_float
                            if ed.get("winner"):
                                is_winner_hist = True
                                break 
                    except (ValueError, TypeError):
                        pass

                if not hq_award_hist and ed.get("winner"):
                    is_winner_hist = True
                    proj_d = project_info_map.get(chosen_project_id)
                    if proj_d:
                        award_from_proj = proj_d.get("total_awards", 0)
                        if award_from_proj > 0:
                            actual_award_hist = award_from_proj
                            hq_award_hist = True
                            break

    # --- 2. åŸºäºå†å²è¡¨ç°è®¡ç®—åŸºç¡€å¥–åŠ± ---
    base_reward = 0.0
    REWARD_ACTION_COST = -0.01 # æ¯æ¬¡è¡ŒåŠ¨çš„åŸºç¡€æˆæœ¬
    final_reward = 0.0  # <--- åœ¨è¿™é‡Œåˆå§‹åŒ– final_reward
    
    if hq_award_hist:
        bonus_awd = (actual_award_hist / scale_reference) if scale_reference > 0 else 0
        base_reward = 1.0 + np.clip(bonus_awd, 0, 1.0)
        final_reward = base_reward * scaling_factor 
    elif worker_participated_hist and hist_score >= 4:
        base_reward = 0.5 + (hist_score - 4) * 0.2
        final_reward = base_reward * scaling_factor 
    elif worker_participated_hist and hist_score >= 3:
        base_reward = 0.2
        final_reward = base_reward * scaling_factor 
    elif not worker_participated_hist:
        # æ½œåŠ›å¥–åŠ±
        numeric_features = chosen_project_state_tuple[3] # æ•°å€¼ç‰¹å¾åœ¨ç´¢å¼•3
        wq_n = numeric_features[0]   # å½’ä¸€åŒ–çš„å·¥äººè´¨é‡
        rps_n = numeric_features[8]  # å½’ä¸€åŒ–çš„å•ä½æ§½ä½å¥–åŠ±
        potential_r = 0.2 * (wq_n + rps_n)
        final_reward = potential_r
    # æœ€ç»ˆçš„å¥–åŠ±æ˜¯ (è¡ŒåŠ¨æˆæœ¬ + åŸºç¡€å¥–åŠ±) * ç¼©æ”¾å› å­
    
    return final_reward
# --- æ–°å¢ï¼šè¯„ä¼°å‡½æ•° (æœ€ç»ˆä¿®æ­£ç‰ˆ) ---
def evaluate_agent(agent_to_eval, evaluation_events, proj_info, ent_info, wq_map, 
                   global_reward_scale_ref):
    
    agent_to_eval.policy_network.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    total_eval_reward = 0
    num_eval_recommendations = 0

    # â­ é‡è¦ä¼˜åŒ–ï¼šåœ¨è¯„ä¼°æ—¶ç¦ç”¨æ¢¯åº¦è®¡ç®—
    with torch.no_grad():
        for event in evaluation_events:
            current_worker_id, current_time_dt = event['worker_id'], event['arrival_time_dt']

            # ... (è·å–å¯ç”¨é¡¹ç›®å’Œæ„å»ºçŠ¶æ€çš„é€»è¾‘ä¿æŒä¸å˜) ...
            available_project_ids = get_available_projects(current_time_dt, proj_info, ent_info)
            if not available_project_ids:
                continue

            state_options_with_proj_id = []
            for proj_id in available_project_ids:
                final_state_tuple = get_new_final_state_tuple(
                    current_worker_id, proj_id, current_time_dt,
                    project_info, worker_quality_map, 
                    worker_global_stats, worker_cat_performance # ä¼ å…¥æ–°çš„ä¸¤ä¸ªç‰¹å¾å­—å…¸
                    )
                if final_state_tuple is not None:
                    state_options_with_proj_id.append((proj_id, final_state_tuple))
            
            if not state_options_with_proj_id:
                continue

            # â­ å…³é”®ä¿®æ­£ï¼šè¯„ä¼°æ—¶ä½¿ç”¨ eps=0.0ï¼Œå®Œå…¨åˆ©ç”¨æ¨¡å‹å­¦åˆ°çš„ç­–ç•¥
            chosen_project_id = agent_to_eval.act(state_options_with_proj_id, eps=0.0)
            if chosen_project_id is None:
                continue

            chosen_project_state_tuple = next(s_tuple for pid, s_tuple in state_options_with_proj_id if pid == chosen_project_id)
            
            # --- è°ƒç”¨ç»Ÿä¸€çš„å¥–åŠ±å‡½æ•° ---
            reward = calculate_unified_reward(
                chosen_project_id=chosen_project_id,
                current_worker_id=current_worker_id,
                chosen_project_state_tuple=chosen_project_state_tuple,
                project_info_map=proj_info, # æ³¨æ„è¿™é‡Œå˜é‡åå« proj_info
                entry_info_map=ent_info,   # æ³¨æ„è¿™é‡Œå˜é‡åå« ent_info
                scale_reference=global_reward_scale_ref, # æ³¨æ„è¿™é‡Œå˜é‡å
                scaling_factor=REWARD_SCALING_FACTOR
            )
            
            total_eval_reward += reward
            num_eval_recommendations += 1

    agent_to_eval.policy_network.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼
    
    avg_eval_reward = total_eval_reward / num_eval_recommendations if num_eval_recommendations > 0 else 0.0
    return avg_eval_reward

# --- å·¥äººåˆ°è¾¾äº‹ä»¶ç”Ÿæˆ (V2ä¿®æ­£ç‰ˆï¼Œæºå¸¦æ›´ä¸°å¯Œä¿¡æ¯) ---
print("\nå¼€å§‹ç”Ÿæˆæ›´ä¸°å¯Œçš„å·¥äººæ´»è·ƒäº‹ä»¶æ±  (æŒ‰å¤©å»é‡)...")
all_event_candidates = []
if entry_info:
    all_potential_events_temp = []
    # é¦–å…ˆæ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„æäº¤äº‹ä»¶
    for project_id, entries_in_project in entry_info.items():
        for entry_number, entry_data in entries_in_project.items():
            if not entry_data.get("withdrawn", False):
                # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šåœ¨äº‹ä»¶ä¸­ä¿å­˜ project_id å’Œ entry_number ---
                all_potential_events_temp.append({
                    'worker_id': entry_data["worker_id"],
                    'project_id': project_id, # <-- ä¿å­˜é¡¹ç›®ID
                    'entry_number': entry_number, # <-- ä¿å­˜æäº¤ç¼–å·
                    'arrival_time_dt': entry_data["entry_created_at_dt"]
                })

    if all_potential_events_temp:
        # æŒ‰å·¥äººå’Œæ—¶é—´æ’åºï¼Œæ–¹ä¾¿åç»­å»é‡
        all_potential_events_temp.sort(key=lambda x: (x['worker_id'], x['arrival_time_dt']))
        
        processed_worker_days = set()
        for event_data in all_potential_events_temp:
            date_str = event_data['arrival_time_dt'].strftime('%Y-%m-%d')
            worker_day_key = (event_data['worker_id'], date_str)
            # ç¡®ä¿æ¯ä¸ªå·¥äººæ¯å¤©åªå–ç¬¬ä¸€ä¸ªäº‹ä»¶
            if worker_day_key not in processed_worker_days:
                all_event_candidates.append(event_data) # æ·»åŠ å®Œæ•´çš„äº‹ä»¶å­—å…¸
                processed_worker_days.add(worker_day_key)

        # æœ€ç»ˆæŒ‰å…¨å±€æ—¶é—´æ’åº
        all_event_candidates.sort(key=lambda x: x['arrival_time_dt'])
        print(f"å»é‡åï¼Œæ€»å…±ç”Ÿæˆäº† {len(all_event_candidates)} ä¸ªå·¥äººæ´»è·ƒäº‹ä»¶ã€‚")
    else:
        print("è­¦å‘Š: æœªèƒ½ä»entry_infoç”Ÿæˆä»»ä½•æ½œåœ¨çš„å·¥äººæ´»è·ƒäº‹ä»¶ã€‚")
else:
    print("è­¦å‘Š: entry_info ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆå·¥äººæ´»è·ƒäº‹ä»¶æ± ã€‚")

# ç¡®ä¿ sorted_all_events ä½¿ç”¨æ–°çš„äº‹ä»¶åˆ—è¡¨
sorted_all_events = all_event_candidates
# --- æ•°æ®é›†åˆ’åˆ† (åŸºäº sorted_all_events) ---
train_events, validation_events, test_events = [], [], []
if sorted_all_events:  # ä½¿ç”¨æ–°çš„ sorted_all_eventsè¿›è¡Œåˆ’åˆ†
    split_ratio_train = 0.7
    split_ratio_val = 0.15

    num_total_events = len(sorted_all_events)

    if num_total_events < 100:
        print("è­¦å‘Š: æ€»äº‹ä»¶æ•°è¿‡å°‘ï¼Œå°†å¤§éƒ¨åˆ†ç”¨äºè®­ç»ƒï¼Œå°‘é‡ç”¨äºéªŒè¯ï¼Œä¸è®¾æµ‹è¯•é›†ã€‚")
        split_idx_train_end = int(num_total_events * 0.85)
        train_events = sorted_all_events[:split_idx_train_end]
        validation_events = sorted_all_events[split_idx_train_end:]
        test_events = []
    else:
        split_ratio_test = 1.0 - split_ratio_train - split_ratio_val  # ç¡®ä¿æ€»å’Œä¸º1

        train_idx_end = int(num_total_events * split_ratio_train)
        val_idx_end = train_idx_end + int(num_total_events * split_ratio_val)

        train_events = sorted_all_events[:train_idx_end]  # ä¿®æ”¹è¿™é‡Œï¼Œç¡®ä¿æ­£ç¡®åˆ‡åˆ†
        validation_events = sorted_all_events[train_idx_end:val_idx_end]
        test_events = sorted_all_events[val_idx_end:]

    print(
        f"æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒé›† {len(train_events)} æ¡, éªŒè¯é›† {len(validation_events)} æ¡, æµ‹è¯•é›† {len(test_events)} æ¡")
else:
    print("è­¦å‘Š: å®Œæ•´å·¥äººæ´»è·ƒäº‹ä»¶æ± ä¸ºç©ºï¼Œæ— æ³•åˆ’åˆ†æ•°æ®é›†ã€‚")
# --- åœ¨ "æ•°æ®é›†åˆ’åˆ†" å’Œ "é¢„è®¡ç®— Min/Max" ä¹‹é—´ï¼ŒåŠ å…¥è¿™æ®µä»£ç  ---
# --- ä¿®æ”¹ï¼šé¢„è®¡ç®— Min/Max ç‰¹å¾å€¼ (ä»…ä½¿ç”¨è®­ç»ƒé›†æ•°æ®) ---
if train_events: # **ç¡®ä¿ train_events ä¸æ˜¯ç©ºçš„**
    precompute_feature_min_max(train_events, project_info, entry_info, worker_quality_map) # **ä¼ å…¥ train_events**
else:
    print("è­¦å‘Š: è®­ç»ƒäº‹ä»¶é›†ä¸ºç©ºï¼Œè·³è¿‡ç‰¹å¾ Min/Max é¢„è®¡ç®—ï¼Œå°†ä½¿ç”¨é»˜è®¤èŒƒå›´(0,1)ã€‚")
    for name in ["worker_quality","time_until_deadline_sec","task_age_sec","project_duration_sec","reward_per_slot","current_time_val"]:
        if name not in feature_min_max: feature_min_max[name] = (0,1)

### --- ä¸»ç¨‹åºæ‰§è¡Œæµç¨‹ --- ###
# å»ºè®®å°†ä¸»æµç¨‹ä»£ç æ”¾åœ¨ if __name__ == "__main__": ä¸­
# è¿™æ˜¯ä¸€ä¸ªPythonç¼–ç¨‹çš„å¥½ä¹ æƒ¯ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨å¤šè¿›ç¨‹ï¼ˆå¦‚DataLoaderçš„num_workers > 0ï¼‰æ—¶ï¼Œå¯ä»¥é¿å…ä¸å¿…è¦çš„é—®é¢˜ã€‚

### --- ä¸»ç¨‹åºæ‰§è¡Œæµç¨‹ (DPOç‰ˆæœ¬) --- ###
if __name__ == "__main__":

    # --- 1. DPOä¸“å±çš„æ•°æ®å‡†å¤‡ ---
    # æ ¸å¿ƒæ”¹åŠ¨ï¼šä¸å†ä½¿ç”¨äº‹ä»¶æµç›´æ¥è®­ç»ƒï¼Œè€Œæ˜¯å…ˆä»äº‹ä»¶æµä¸­ç”ŸæˆDPOæ‰€éœ€çš„åå¥½å¯¹ã€‚
    # æ³¨æ„ï¼šè¿™ä¸ªè¿‡ç¨‹å¯èƒ½ä¼šèŠ±è´¹ä¸€äº›æ—¶é—´ï¼Œæ˜¯å¿…è¦çš„ä¸€æ¬¡æ€§é¢„å¤„ç†ã€‚
    dpo_train_data = create_preference_dataset_v4(
        train_events, project_info, entry_info, worker_quality_map, 
        worker_global_stats, worker_cat_performance
    )
    dpo_val_data = create_preference_dataset_v4(
        validation_events, project_info, entry_info, worker_quality_map, 
        worker_global_stats, worker_cat_performance
    )

    # å¦‚æœç”Ÿæˆçš„æ•°æ®è¿‡å°‘ï¼Œåç»­å¯èƒ½æ— æ³•è®­ç»ƒ
    if len(dpo_train_data) < BATCH_SIZE:
        print(f"é”™è¯¯ï¼šç”Ÿæˆçš„è®­ç»ƒåå¥½æ•°æ®ï¼ˆ{len(dpo_train_data)}æ¡ï¼‰ä¸è¶³ä¸€ä¸ªæ‰¹æ¬¡ï¼ˆ{BATCH_SIZE}æ¡ï¼‰ï¼Œæ— æ³•ç»§ç»­ã€‚")
        sys.exit() # é€€å‡ºç¨‹åº

    # --- 2. å®ä¾‹åŒ– DPO Agent ---
    # æ ¸å¿ƒæ”¹åŠ¨ï¼šä½¿ç”¨æˆ‘ä»¬æ–°å®šä¹‰çš„ DPOAgent
    agent = DPOAgent(
        num_categories_embed_size=NUM_CATEGORIES_EMBED_SIZE,
        num_sub_categories_embed_size=NUM_SUB_CATEGORIES_EMBED_SIZE,
        num_industries_embed_size=NUM_INDUSTRIES_EMBED_SIZE,
        cat_embed_dim=CATEGORY_EMBED_DIM,
        sub_cat_embed_dim=SUB_CATEGORY_EMBED_DIM,
        ind_embed_dim=INDUSTRY_EMBED_DIM,
        total_numeric_features=TOTAL_NUMERIC_FEATURES, 
        seed=0
    )

    # DPOä¸éœ€è¦å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œå¯ä»¥å…ˆç§»é™¤
    # scheduler = optim.lr_scheduler.StepLR(agent.optimizer, step_size=40, gamma=0.5)

    # --- 3. åˆ›å»ºæ–°çš„ DataLoader ---
    # æ ¸å¿ƒæ”¹åŠ¨ï¼šä¸ºDPOåå¥½å¯¹æ•°æ®åˆ›å»ºä¸€ä¸ªæ–°çš„ã€æ›´æ ‡å‡†çš„DataLoader
    def dpo_collate_fn(batch):
        """ä¸€ä¸ªç®€å•çš„æ•´ç†å‡½æ•°ï¼Œç”¨äºå°†åå¥½å¯¹åˆ—è¡¨æ‰“åŒ…æˆtensoræ‰¹æ¬¡ã€‚"""
        chosen_id_list, chosen_num_list = [], []
        rejected_id_list, rejected_num_list = [], []
        
        for pair in batch:
            chosen_s, rejected_s = pair
            chosen_id_list.append([chosen_s[0], chosen_s[1], chosen_s[2]])
            chosen_num_list.append(chosen_s[3])
            rejected_id_list.append([rejected_s[0], rejected_s[1], rejected_s[2]])
            rejected_num_list.append(rejected_s[3])
            
        chosen_ids = torch.LongTensor(chosen_id_list)
        chosen_nums = torch.FloatTensor(np.array(chosen_num_list))
        rejected_ids = torch.LongTensor(rejected_id_list)
        rejected_nums = torch.FloatTensor(np.array(rejected_num_list))
        
        # è¿”å›ä¸¤ä¸ªå…ƒç»„ï¼Œæ¯ä¸ªå…ƒç»„åŒ…å«(id_features, numeric_features)
        return (chosen_ids, chosen_nums), (rejected_ids, rejected_nums)

    train_loader = DataLoader(
        dpo_train_data, 
        batch_size=BATCH_SIZE,
        shuffle=True,       # DPOä½¿ç”¨æ ‡å‡†ç›‘ç£å­¦ä¹ ï¼Œæ‰“ä¹±æ•°æ®æ˜¯å¥½ä¹ æƒ¯
        num_workers=4,
        collate_fn=dpo_collate_fn,
        pin_memory=True
    )
    
    # --- 4. åˆå§‹åŒ–è®­ç»ƒæ‰€éœ€å˜é‡ ---
    num_epochs = 120  # DPOå¯èƒ½æ”¶æ•›æ›´å¿«ï¼Œå¯é€‚å½“è°ƒæ•´
    all_train_epoch_losses = []
    all_val_epoch_rewards = []
    
    print("\nå¼€å§‹ä½¿ç”¨DPOè¿›è¡Œè®­ç»ƒ...")
    
    # æ—©åœæœºåˆ¶å˜é‡ (ä¿ç•™ï¼Œä½†ä¿®æ”¹æ–‡ä»¶åä»¥ä½œåŒºåˆ†)
    best_val_reward = -float('inf')
    script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()
    best_model_path = os.path.join(script_dir, 'dpo_best_model.pth')
    patience = 10 # DPOè®­ç»ƒå¯èƒ½æ›´ç¨³å®šï¼Œå¯ä»¥é€‚å½“å¢åŠ è€å¿ƒ
    epochs_no_improve = 0

    # --- 5. å¼€å§‹æ–°çš„DPOä¸»è®­ç»ƒå¾ªç¯ ---
    for i_epoch in range(1, num_epochs + 1):
        agent.policy_network.train() # ç¡®ä¿æ˜¯è®­ç»ƒæ¨¡å¼
        epoch_losses = []
        
        # æ ¸å¿ƒæ”¹åŠ¨ï¼šç›´æ¥ä»DataLoaderä¸­è·å–é¢„å¤„ç†å¥½çš„æ‰¹æ¬¡
        for batch in train_loader:
            # å°†æ•°æ®ç§»åŠ¨åˆ°GPU
            chosen_states, rejected_states = batch
            chosen_states_gpu = (chosen_states[0].to(device, non_blocking=True), chosen_states[1].to(device, non_blocking=True))
            rejected_states_gpu = (rejected_states[0].to(device, non_blocking=True), rejected_states[1].to(device, non_blocking=True))
            
            loss = agent.learn((chosen_states_gpu, rejected_states_gpu))
            epoch_losses.append(loss)
            
        avg_loss = np.mean(epoch_losses)
        all_train_epoch_losses.append(avg_loss)
        
        # --- 6. éªŒè¯ã€è¯„ä¼°ä¸æ—©åœ ---
        # è¯„ä¼°éƒ¨åˆ†æˆ‘ä»¬ä»ç„¶æ²¿ç”¨ä¹‹å‰çš„ evaluate_agent å‡½æ•°ï¼Œå› ä¸ºå®ƒè¡¡é‡çš„æ˜¯åœ¨çœŸå®æ¨èåœºæ™¯ä¸‹çš„ä¸šåŠ¡å¥–åŠ±ï¼Œ
        # è¿™æ˜¯æ¯”DPOè‡ªèº«çš„â€œåˆ†ç±»å‡†ç¡®ç‡â€æ›´æœ‰ä»·å€¼ã€ä¹Ÿæ›´æ–¹ä¾¿ä¸CQLå¯¹æ¯”çš„æŒ‡æ ‡ã€‚
        if i_epoch % 5 == 0 and dpo_val_data: # å¯ä»¥æ›´é¢‘ç¹åœ°éªŒè¯
            val_reward_epoch = evaluate_agent(agent, validation_events, project_info, entry_info, worker_quality_map, REWARD_SCALE_REFERENCE)
            all_val_epoch_rewards.append(val_reward_epoch)
            
            print(f"è½®æ¬¡ {i_epoch}/{num_epochs}\tè®­ç»ƒæŸå¤±: {avg_loss:.6f}\téªŒè¯å¥–åŠ±: {val_reward_epoch:.4f}")

            # æ—©åœé€»è¾‘ (åŸºæœ¬ä¸å˜ï¼Œåªéœ€ä¿®æ”¹ä¿å­˜çš„æ¨¡å‹)
            if val_reward_epoch > best_val_reward:
                best_val_reward = val_reward_epoch
                epochs_no_improve = 0
                try:
                # æ£€æŸ¥ agent.policy_network æ˜¯å¦ä¸º DataParallel å®ä¾‹
                    if isinstance(agent.policy_network, nn.DataParallel):
                    # å¦‚æœæ˜¯ï¼Œåˆ™æˆ‘ä»¬åªä¿å­˜å…¶å†…éƒ¨çš„ .module çš„çŠ¶æ€å­—å…¸
                        state_dict_to_save = agent.policy_network.module.state_dict()
                    else:
                    # å¦‚æœä¸æ˜¯ï¼ˆä¾‹å¦‚åœ¨å•GPUç¯å¢ƒä¸‹è¿è¡Œï¼‰ï¼Œåˆ™ç›´æ¥ä¿å­˜
                        state_dict_to_save = agent.policy_network.state_dict()
    
                    torch.save(state_dict_to_save, best_model_path)
                    print(f"  ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯å¥–åŠ±ï¼æ¨¡å‹å·²ä¿å­˜åˆ° {best_model_path}")

                except Exception as e_save_best:
                    print(f"  ä¿å­˜æœ€ä½³æ¨¡å‹å¤±è´¥: {e_save_best}")

            else:
                epochs_no_improve += 1

            if epochs_no_improve >= patience:
                print(f"\néªŒè¯å¥–åŠ±å·²è¿ç»­ {patience * 5} ä¸ªè½®æ¬¡æ²¡æœ‰æå‡ï¼Œè§¦å‘æå‰åœæ­¢ã€‚")
                break
    
    print("\nè®­ç»ƒå®Œæˆã€‚")

    # --- 7. æœ€ç»ˆè¯„ä¼°ã€ç»˜å›¾ä¸ä¿å­˜ ---
    # --- åœ¨ "7. æœ€ç»ˆè¯„ä¼°ã€ç»˜å›¾ä¸ä¿å­˜" éƒ¨åˆ† ---
    if test_events:
        print("\nå¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°DPOæœ€ä½³æ¨¡å‹...")
        try:
            if os.path.exists(best_model_path):
                print(f"æ­£åœ¨ä» {best_model_path} åŠ è½½æœ€ä½³æ¨¡å‹...")
                # åŠ è½½æˆ‘ä»¬ä¿å­˜çš„â€œå¹²å‡€â€çš„çŠ¶æ€å­—å…¸
                state_dict = torch.load(best_model_path, map_location=device)
            
                # è·å–æ ¸å¿ƒæ¨¡å‹ï¼ˆæ— è®ºæ˜¯å•GPUè¿˜æ˜¯å¤šGPUï¼‰
                model_to_load = agent.policy_network.module if isinstance(agent.policy_network, nn.DataParallel) else agent.policy_network
            
                # å°†çŠ¶æ€å­—å…¸åŠ è½½åˆ°æ ¸å¿ƒæ¨¡å‹ä¸­
                model_to_load.load_state_dict(state_dict)
            
                print("DPOæœ€ä½³æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
                test_reward = evaluate_agent(agent, test_events, project_info, entry_info, worker_quality_map, REWARD_SCALE_REFERENCE)
                print(f"ğŸ† DPOæ¨¡å‹ - æµ‹è¯•é›†å¹³å‡å¥–åŠ±: {test_reward:.4f}")
            else:
                print("è­¦å‘Š: æœªæ‰¾åˆ°DPOæœ€ä½³æ¨¡å‹æ–‡ä»¶ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•ã€‚")
                test_reward = None
        except Exception as e_test:
            print(f"DPOæ¨¡å‹æµ‹è¯•è¯„ä¼°å¤±è´¥: {e_test}")
            test_reward = None

    try:
        plt.figure(figsize=(16, 8))
        
        # å¥–åŠ±å›¾
        ax1 = plt.subplot(1, 2, 1)
        # DPOæ²¡æœ‰è®­ç»ƒå¥–åŠ±ï¼Œåªç»˜åˆ¶éªŒè¯å¥–åŠ±
        if all_val_epoch_rewards:
            val_epochs_plot = range(5, 5 * len(all_val_epoch_rewards) + 1, 5)
            ax1.plot(val_epochs_plot, all_val_epoch_rewards, label='Avg Reward per Validation Epoch', marker='o', linestyle='--')
        if test_reward is not None:
            ax1.axhline(y=test_reward, color='r', linestyle=':', linewidth=2, label=f'Test Set Avg Reward: {test_reward:.4f}')
        ax1.set_xlabel('Epoch'); ax1.set_ylabel('Average Reward'); ax1.set_title('DPO Validation & Test Reward'); ax1.legend(); ax1.grid(True)
        
        # æŸå¤±å›¾
        ax2 = plt.subplot(1, 2, 2)
        ax2.plot(all_train_epoch_losses, label='Avg DPO Loss per Training Epoch', color='green')
        ax2.set_xlabel('Epoch'); ax2.set_ylabel('Loss'); ax2.set_title('DPO Training: Average Loss per Epoch'); ax2.legend(); ax2.grid(True)
        
        plt.tight_layout()
        curves_save_path = os.path.join(script_dir, 'dpo_training_curves.png')
        plt.savefig(curves_save_path)
        print(f"\nDPOè®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° {curves_save_path}")
    except Exception as e_plot:
        print(f"\nç»˜åˆ¶DPOè®­ç»ƒæ›²çº¿å¤±è´¥: {e_plot}")